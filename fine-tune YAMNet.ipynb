{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "os.sys.path.append('./audioset/yamnet')\n",
    "import yamnet as yamnet_model\n",
    "import params\n",
    "from datagen_yamnet import DataGenerator, get_files_and_labels\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# Directory storing the spectrogram inputs for each class\n",
    "train_dir = './train_set_patches/'\n",
    "\n",
    "# Path to output model\n",
    "model_out = './saved_models/model'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get training data files and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'train_split' parameter below sets how much of the training data will be randomly sampled for test data\n",
    "files_train, labels_train, files_val, labels_val, class_dict = get_files_and_labels(train_dir, \n",
    "                                                                                    typ='npy',\n",
    "                                                                                    train_split=0.9)\n",
    "\n",
    "# If you want to use your own separate folder with test samples instead, do this:\n",
    "# files_train, labels_train, _, _, class_dict = get_files_and_labels(train_dir, \n",
    "#                                                                     typ='npy',\n",
    "#                                                                     train_split=1)\n",
    "# files_val, labels_val, _, _, _ = get_files_and_labels(test_dir, \n",
    "#                                                         typ='npy',\n",
    "#                                                         train_split=1)\n",
    "\n",
    "class_dict_rev = {(str(v[0])): k for k, v in class_dict.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pre-trained YAMNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yamnet = yamnet_model.yamnet_model()\n",
    "yamnet.load_weights('./audioset/yamnet/yamnet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yamnet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define custom \"top\" of YAMNet graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inpts = tf.keras.layers.Input(shape=(params.PATCH_FRAMES, params.PATCH_BANDS))\n",
    "\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(yamnet.layers[-3].output)\n",
    "o = tf.keras.layers.Dropout(0.5)(x)\n",
    "o = tf.keras.layers.Dense(2, activation='softmax')(o)\n",
    "\n",
    "model = Model(inputs=yamnet.input, outputs=o)\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.0001, decay=1e-7)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_generator = DataGenerator(files_train,\n",
    "                                labels_train,\n",
    "                                batch_size=batch_size)\n",
    "validation_generator = DataGenerator(files_val,\n",
    "                                    labels_val,\n",
    "                                    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Save model architecture\n",
    "model_json = model.to_json()\n",
    "with open(model_out+'.json', \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "with open(model_out+'_classes.json', 'w') as f:\n",
    "    json.dump(class_dict, f)\n",
    "print('Saved model architecture')\n",
    "\n",
    "# Define training callbacks\n",
    "checkpoint = ModelCheckpoint(model_out+'.h5',\n",
    "                             monitor='val_loss', \n",
    "                             verbose=1,\n",
    "                             save_best_only=True, \n",
    "                             mode='auto')\n",
    "\n",
    "reducelr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                              factor=0.5, \n",
    "                              patience=3, \n",
    "                              verbose=1)\n",
    "\n",
    "# Compile model\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model_history = model.fit(train_generator,\n",
    "                                steps_per_epoch = len(train_generator),\n",
    "                                epochs = 20,\n",
    "                                validation_data = validation_generator,\n",
    "                                validation_steps = len(validation_generator),\n",
    "                                verbose = 1,\n",
    "                                callbacks=[checkpoint, reducelr])\n",
    "except Exception as e:\n",
    "    err = e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
